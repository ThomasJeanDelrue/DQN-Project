{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.7"
    },
    "colab": {
      "name": "DQN_project_MVA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QYg4i5Kl3Lk",
        "colab_type": "text"
      },
      "source": [
        "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vIfG_USm9nx",
        "colab_type": "code",
        "outputId": "9e505fb7-5bb3-477f-cf0d-250a589b3701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!pip install sk-video"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sk-video in /usr/local/lib/python3.6/dist-packages (1.1.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sk-video) (1.17.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sk-video) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lajcoqxIl3Lr",
        "colab_type": "code",
        "outputId": "8fc9ce84-cd5e-407d-b4b3-e69be05b211e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "import skvideo.io\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import sgd, adam\n",
        "from keras import regularizers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization, Flatten"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3KVnm-wl3Lz",
        "colab_type": "text"
      },
      "source": [
        "# MiniProject on Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apwb6rERl3L1",
        "colab_type": "text"
      },
      "source": [
        "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8ziR9kkl3L3",
        "colab_type": "text"
      },
      "source": [
        "# Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTd4oqdvl3L4",
        "colab_type": "text"
      },
      "source": [
        "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
        "\n",
        "\\begin{equation*}\n",
        "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
        "\\end{equation*}\n",
        "\n",
        "where: \n",
        "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
        "\\end{equation*}\n",
        "\n",
        "We note the $Q$-function:\n",
        "\n",
        "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
        "\\end{equation*}\n",
        "\n",
        "Thus, the optimal Q function is:\n",
        "\\begin{equation*}\n",
        "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
        "\\end{equation*}\n",
        "\n",
        "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-7c9uYBl3L7",
        "colab_type": "text"
      },
      "source": [
        "## The environment, the agent and the game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF53VaC8l3L9",
        "colab_type": "text"
      },
      "source": [
        "### The environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_KSlicCl3L-",
        "colab_type": "text"
      },
      "source": [
        "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo-X_yyOl3MA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment(object):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def act(self, act):\n",
        "        \"\"\"\n",
        "        One can act on the environment and obtain its reaction:\n",
        "        - the new state\n",
        "        - the reward of the new state\n",
        "        - should we continue the game?\n",
        "\n",
        "        :return: state, reward, game_over\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reinitialize the environment to a random state and returns\n",
        "        the original state\n",
        "\n",
        "        :return: state\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def draw(self):\n",
        "        \"\"\"\n",
        "        Visualize in the console or graphically the current state\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtuIziDwl3MF",
        "colab_type": "text"
      },
      "source": [
        "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
        "\n",
        "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
        "\n",
        "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
        "\n",
        "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp_hNbDHl3MH",
        "colab_type": "text"
      },
      "source": [
        "### The Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDx29DC7l3MJ",
        "colab_type": "text"
      },
      "source": [
        "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjnFaiCMl3ML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, epsilon=0.1, n_action=4):\n",
        "        self.epsilon = epsilon\n",
        "        self.n_action = n_action\n",
        "    \n",
        "    def set_epsilon(self,e):\n",
        "        self.epsilon = e\n",
        "\n",
        "    def act(self,s,train=True):\n",
        "        \"\"\" This function should return the next action to do:\n",
        "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
        "        if train:\n",
        "            if np.random.rand() <= self.epsilon:\n",
        "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
        "            else:\n",
        "                a = self.learned_act(s)\n",
        "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
        "            a = self.learned_act(s)\n",
        "\n",
        "        return a\n",
        "\n",
        "    def learned_act(self,s):\n",
        "        \"\"\" Act via the policy of the agent, from a given state s\n",
        "        it proposes an action a\"\"\"\n",
        "        pass\n",
        "\n",
        "    def reinforce(self, s, n_s, a, r, game_over_):\n",
        "        \"\"\" This function is the core of the learning algorithm. \n",
        "        It takes as an input the current state s_, the next state n_s_\n",
        "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
        "        \n",
        "        Its goal is to learn a policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\" This function returns basic stats if applicable: the\n",
        "        loss and/or the model\"\"\"\n",
        "        pass\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\" This function allows to restore a model\"\"\"\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_KvD2vMl3MR",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "__Question 1__:\n",
        "Explain the function act. Why is ```epsilon``` essential?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdtSPqvMwL80",
        "colab_type": "text"
      },
      "source": [
        "```act``` returns action $a_t$ for the environment state $s_t$.\n",
        "It will return:\n",
        "\n",
        "*   a random action, with proba $\\varepsilon$\n",
        "*   the best action learned so far to maximize $r(s_t, .)$, with proba $1-\\varepsilon$\n",
        "\n",
        "If $\\varepsilon$ is too small, it will take a long time to explore the different possibilities and to find the optimal action for each state.\n",
        "On the contrary, if $\\varepsilon$ is too large, the agent will not act optimally in the long run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb5UbLG9l3MU",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "### The Game"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qBvrkCkl3MW",
        "colab_type": "text"
      },
      "source": [
        "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
        "\n",
        "```python\n",
        "\n",
        "epoch = 300\n",
        "env = Environment()\n",
        "agent = Agent()\n",
        "\n",
        "\n",
        "# Number of won games\n",
        "score = 0\n",
        "loss = 0\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "    # At each epoch, we restart to a fresh game and get the initial state\n",
        "    state = env.reset()\n",
        "    # This assumes that the games will end\n",
        "    game_over = False\n",
        "\n",
        "    win = 0\n",
        "    lose = 0\n",
        "    \n",
        "    while not game_over:\n",
        "        # The agent performs an action\n",
        "        action = agent.act(state)\n",
        "\n",
        "        # Apply an action to the environment, get the next state, the reward\n",
        "        # and if the games end\n",
        "        prev_state = state\n",
        "        state, reward, game_over = env.act(action)\n",
        "\n",
        "        # Update the counters\n",
        "        if reward > 0:\n",
        "            win = win + reward\n",
        "        if reward < 0:\n",
        "            lose = lose -reward\n",
        "\n",
        "        # Apply the reinforcement strategy\n",
        "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
        "\n",
        "    # Save as a mp4\n",
        "    if e % 10 == 0:\n",
        "        env.draw(e)\n",
        "\n",
        "    # Update stats\n",
        "    score += win-lose\n",
        "\n",
        "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
        "          .format(e, epoch, loss, win, lose, win-lose))\n",
        "    agent.save()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f-WO1eSl3MY",
        "colab_type": "text"
      },
      "source": [
        "# The game, *eat cheese*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGw_p8S0l3Ma",
        "colab_type": "text"
      },
      "source": [
        "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
        "\n",
        "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lkVooZzl3Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment(object):\n",
        "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
        "        grid_size = grid_size+4\n",
        "        self.grid_size = grid_size\n",
        "        self.max_time = max_time\n",
        "        self.temperature = temperature\n",
        "\n",
        "        #board on which one plays\n",
        "        self.board = np.zeros((grid_size,grid_size))\n",
        "        self.position = np.zeros((grid_size,grid_size))\n",
        "\n",
        "        # coordinate of the cat\n",
        "        self.x = 0\n",
        "        self.y = 1\n",
        "\n",
        "        # self time\n",
        "        self.t = 0\n",
        "\n",
        "        self.scale=16\n",
        "\n",
        "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
        "\n",
        "\n",
        "    def draw(self,e):\n",
        "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
        "\n",
        "    def get_frame(self,t):\n",
        "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
        "        b[self.board>0,0] = 256\n",
        "        b[self.board < 0, 2] = 256\n",
        "        b[self.x,self.y,:]=256\n",
        "        b[-2:,:,:]=0\n",
        "        b[:,-2:,:]=0\n",
        "        b[:2,:,:]=0\n",
        "        b[:,:2,:]=0\n",
        "        \n",
        "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        self.to_draw[t,:,:,:]=b\n",
        "\n",
        "\n",
        "    def act(self, action):\n",
        "        \"\"\"This function returns the new state, reward and decides if the\n",
        "        game ends.\"\"\"\n",
        "\n",
        "        self.get_frame(int(self.t))\n",
        "\n",
        "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
        "\n",
        "        self.position[0:2,:]= -1\n",
        "        self.position[:,0:2] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "\n",
        "        self.position[self.x, self.y] = 1\n",
        "        if action == 0:\n",
        "            if self.x == self.grid_size-3:\n",
        "                self.x = self.x-1\n",
        "            else:\n",
        "                self.x = self.x + 1\n",
        "        elif action == 1:\n",
        "            if self.x == 2:\n",
        "                self.x = self.x+1\n",
        "            else:\n",
        "                self.x = self.x-1\n",
        "        elif action == 2:\n",
        "            if self.y == self.grid_size - 3:\n",
        "                self.y = self.y - 1\n",
        "            else:\n",
        "                self.y = self.y + 1\n",
        "        elif action == 3:\n",
        "            if self.y == 2:\n",
        "                self.y = self.y + 1\n",
        "            else:\n",
        "                self.y = self.y - 1\n",
        "        else:\n",
        "            RuntimeError('Error: action not recognized')\n",
        "\n",
        "        self.t = self.t + 1\n",
        "        reward = self.board[self.x, self.y]\n",
        "        self.board[self.x, self.y] = 0\n",
        "        game_over = self.t > self.max_time\n",
        "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
        "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
        "\n",
        "        return state, reward, game_over\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
        "\n",
        "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
        "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
        "\n",
        "\n",
        "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
        "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
        "\n",
        "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
        "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
        "\n",
        "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
        "\n",
        "\n",
        "        malus[bonus>0]=0\n",
        "\n",
        "        self.board = bonus + malus\n",
        "\n",
        "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.position[0:2,:]= -1\n",
        "        self.position[:,0:2] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        self.position[-2:, :] = -1\n",
        "        self.board[self.x,self.y] = 0\n",
        "        self.t = 0\n",
        "\n",
        "        state = np.concatenate((\n",
        "                               self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
        "\n",
        "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
        "        return state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_1NzR70l3Mg",
        "colab_type": "text"
      },
      "source": [
        "The following elements are important because they correspond to the hyper parameters for this project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAhOmzBal3Mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "size = 13\n",
        "T=200\n",
        "temperature=0.3\n",
        "epochs_train=35 # set small when debugging\n",
        "epochs_test=15 # set small when debugging\n",
        "\n",
        "# display videos\n",
        "def display_videos(name):\n",
        "    video = io.open(name, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    return '''<video alt=\"test\" controls>\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_WHUiMQl3Mq",
        "colab_type": "text"
      },
      "source": [
        "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJaeKyy2l3Mr",
        "colab_type": "text"
      },
      "source": [
        "```position``` = the spatial map of the island, with following values\n",
        "\n",
        "\n",
        "*   -1 : sea\n",
        "*   0 : unvisited cell\n",
        "*   1 : visited cell\n",
        "\n",
        "```board``` = the reward map of the island, with following values\n",
        "\n",
        "\n",
        "\n",
        "*   -1 : poison\n",
        "*    0 : reward already received\n",
        "*   +0.5 : cheese\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juhQMor7l3Ms",
        "colab_type": "text"
      },
      "source": [
        "## Random Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YOCUbD7l3Mt",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgvxSmaJl3Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomAgent(Agent):\n",
        "    def __init__(self):\n",
        "        super(RandomAgent, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def learned_act(self, s):\n",
        "        return np.random.randint(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXXSXJCsl3M0",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "***\n",
        "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7tO7Q_Kl3M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(agent,env,epochs,prefix=''):\n",
        "    # Number of won games\n",
        "    score = 0\n",
        "        \n",
        "    for e in range(epochs):\n",
        "\n",
        "        # Initialize the game\n",
        "        state = env.reset()\n",
        "        win = 0\n",
        "        lose = 0\n",
        "        game_over = False\n",
        "\n",
        "        while not game_over:\n",
        "          action = agent.act(state)\n",
        "          state, reward, game_over = env.act(action)\n",
        "\n",
        "          # Update the counters\n",
        "          if reward > 0:\n",
        "              win = win + reward\n",
        "          if reward < 0:\n",
        "              lose = lose - reward\n",
        "        \n",
        "        # Save as a mp4\n",
        "        env.draw(prefix+str(e))\n",
        "\n",
        "        # Update stats\n",
        "        score = score + win-lose\n",
        "\n",
        "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
        "              .format(win, lose, score/(1+e)))\n",
        "    print('Final score: '+str(score/epochs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oct9g7W6l3M9",
        "colab_type": "code",
        "outputId": "c1754c39-bba1-4739-db54-5491cd91b891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "# Initialize the game\n",
        "env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
        "\n",
        "# Initialize the agent!\n",
        "agent = RandomAgent()\n",
        "\n",
        "test(agent,env,epochs_test,prefix='random')\n",
        "HTML(display_videos('random0.mp4'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Win/lose count 12.5/10.0. Average score (2.5)\n",
            "Win/lose count 12.0/16.0. Average score (-0.75)\n",
            "Win/lose count 8.5/6.0. Average score (0.3333333333333333)\n",
            "Win/lose count 7.0/3.0. Average score (1.25)\n",
            "Win/lose count 11.5/17.0. Average score (-0.1)\n",
            "Win/lose count 10.0/13.0. Average score (-0.5833333333333334)\n",
            "Win/lose count 9.0/11.0. Average score (-0.7857142857142857)\n",
            "Win/lose count 8.5/7.0. Average score (-0.5)\n",
            "Win/lose count 9.5/13.0. Average score (-0.8333333333333334)\n",
            "Win/lose count 12.0/4.0. Average score (0.05)\n",
            "Win/lose count 8.0/18.0. Average score (-0.8636363636363636)\n",
            "Win/lose count 12.0/9.0. Average score (-0.5416666666666666)\n",
            "Win/lose count 6.0/11.0. Average score (-0.8846153846153846)\n",
            "Win/lose count 6.0/13.0. Average score (-1.3214285714285714)\n",
            "Win/lose count 6.5/11.0. Average score (-1.5333333333333334)\n",
            "Final score: -1.5333333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5zidQFVl3ND",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGl7wDxzl3NE",
        "colab_type": "text"
      },
      "source": [
        "Let us assume here that $T=\\infty$.\n",
        "\n",
        "***\n",
        "__Question 5__ Let $\\pi$ be a policy, show that:\n",
        "\n",
        "\\begin{equation*}\n",
        "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
        "\\end{equation*}\n",
        "\n",
        "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
        "\n",
        "\\begin{equation*}\n",
        "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
        "\\end{equation*}\n",
        "Finally, deduce that a plausible objective is:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
        "\\end{equation*}\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52txhALAl3NF",
        "colab_type": "text"
      },
      "source": [
        "__Question 5.a__ By definition\n",
        "\n",
        "\\begin{align*}\n",
        "Q^{\\pi}(s,a) \n",
        "& = E_{p^{\\pi}}[\\sum_{t=0}^\\infty\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\\\\n",
        "& = E_{p^{\\pi}}[r(s_0,a_0) + \\sum_{t=1}^\\infty\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\\\\n",
        "& = r(s,a) + E_{p^{\\pi}}[\\gamma \\sum_{t=0}^\\infty\\gamma^{t}r(s_{t+1},a_{t+1})|s_{0}=s,a_{0}=a] \\\\\n",
        "& := r(s,a) + h(s,a)\n",
        "\\end{align*}\n",
        "\n",
        "\\begin{equation*}\n",
        "\\end{equation*}\n",
        "\n",
        "Then $E_{p^{\\pi}}[\\gamma \\sum_{t=0}^\\infty\\gamma^{t}r(s_{t+1},a_{t+1})|s_{0},a_{0}] = h(s_0,a_0)$\n",
        "\n",
        "\\begin{equation*}\n",
        "\\end{equation*}\n",
        "\n",
        "Since $\\sigma(s_0, a_0) \\subset \\sigma(s_0, a_0, s_1, a_1)$ we have\n",
        "\n",
        "\\begin{align*}\n",
        "h(s_0,a_0)\n",
        "& = E_{p^{\\pi}}[E_{p^{\\pi}}[\\gamma \\sum_{t=0}^\\infty\\gamma^{t}r(s_{t+1},a_{t+1})|s_{0},a_{0},s_{1},a_{1}]|s_{0},a_{0}]\n",
        "\\end{align*}\n",
        "\n",
        "We apply the Markov property to get\n",
        "\n",
        "\\begin{align*}\n",
        "h(s_0,a_0)\n",
        "& = E_{p^{\\pi}}[E_{p^{\\pi}}[\\gamma \\sum_{t=0}^\\infty\\gamma^{t}r(s_{t+1},a_{t+1})|s_{1},a_{1}]|s_{0},a_{0}] \\\\\n",
        "& = E_{p^{\\pi}}[\\gamma \\sum_{s',a'}E_{p^{\\pi}}[ \\sum_{t=0}^\\infty\\gamma^{t}r(s_{t+1},a_{t+1})|s_{1}=s',a_{1}=a']\\mathbf{1}_{s_{1}=s',a_{1}=a'}|s_{0},a_{0}] \\\\\n",
        "& = E_{p^{\\pi}}[\\gamma \\sum_{s',a'}E_{p^{\\pi}}[ \\sum_{t=0}^\\infty\\gamma^{t}r(s_{t},a_{t})|s_{0}=s',a_{0}=a']\\mathbf{1}_{s_{1}=s',a_{1}=a'}|s_{0},a_{0}] \\\\\n",
        "& = E_{p^{\\pi}}[\\gamma \\sum_{s',a'}Q^{\\pi}(s',a')\\mathbf{1}_{s_{1}=s',a_{1}=a'}|s_{0},a_{0}] \\\\\n",
        "& = \\sum_{s',a'} \\gamma Q^{\\pi}(s',a') E_{p^{\\pi}}[\\mathbf{1}_{s_{1}=s',a_{1}=a'}|s_{0},a_{0}]\n",
        "\\end{align*}\n",
        "\n",
        "Therefore\n",
        "\n",
        "\\begin{align*}\n",
        "h(s,a)\n",
        "& = \\sum_{s',a'} \\gamma Q^{\\pi}(s',a') E_{p^{\\pi}}[\\mathbf{1}_{s_{1}=s',a_{1}=a'}|s_{0}=s,a_{0}=a] \\\\\n",
        "& = \\sum_{s',a'} \\gamma Q^{\\pi}(s',a') p^{\\pi}(s_{1}=s',a_{1}=a'|s_{0}=s,a_{0}=a) \\\\\n",
        "& = E_{(s',a')\\sim p(.|s,a)}[\\gamma Q^{\\pi}(s',a')]\n",
        "\\end{align*}\n",
        "\n",
        "And finally\n",
        "\n",
        "\\begin{align*}\n",
        "Q^{\\pi}(s,a) \n",
        "& = r(s,a) + h(s,a) \\\\\n",
        "& = r(s,a) + E_{(s',a')\\sim p(.|s,a)}[\\gamma Q^{\\pi}(s',a')] \\\\\n",
        "& = E_{(s',a')\\sim p(.|s,a)}[r(s,a) + \\gamma Q^{\\pi}(s',a')]\n",
        "\\end{align*}\n",
        "\n",
        "***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL1lGRyqkyks",
        "colab_type": "text"
      },
      "source": [
        "__Question 5.b__\n",
        "\n",
        "We assume the existence of an optimal policy $\\pi^*$ that verifies\n",
        "\\begin{equation*}\n",
        "\\pi^*(s') = arg\\max_{a'} Q^{*}(s',a') \\quad \\forall s'\n",
        "\\end{equation*}\n",
        "\n",
        "Then we take $Q=Q^*$ in the above equation\n",
        "\n",
        "\\begin{align*}\n",
        "Q^*(s,a)\n",
        "& = r(s,a) + E_{(s',a')\\sim p(.|s,a)}[\\gamma Q^{*}(s',a')] \\\\\n",
        "& = r(s,a) + E_{s' \\sim \\pi^*(.|s,a)\\,\\, a'=\\pi^*(s'|s,a)}[\\gamma Q^{*}(s',a')]\\\\\n",
        "& = r(s,a) + E_{s' \\sim \\pi^*(.|s,a)}[\\gamma Q^{*}(s',\\pi^*(s'|s,a))]\\\\\n",
        "& = E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')]\n",
        "\\end{align*}\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic9B9GNvUBJd",
        "colab_type": "text"
      },
      "source": [
        "__Question 5.c__\n",
        "\n",
        "$\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}$\n",
        "\n",
        "We just proved that\n",
        "\n",
        "\\begin{equation*}\n",
        "Q^*(s,a)  = E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')]\n",
        "\\end{equation*}\n",
        "\n",
        "For $Q(s,a,\\theta) = Q^*(s,a)$ we have \n",
        "\n",
        "\n",
        "\\begin{equation*}\n",
        "E_{s' \\sim \\pi^*(.|s,a)}[ r+\\gamma\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)] = 0\n",
        "\\end{equation*}\n",
        "\n",
        "\n",
        "Then $\\mathcal{L}$ is a plausible objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmFuagJ-l3NG",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
        "\n",
        "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
        "\n",
        "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
        "\n",
        "3. Store $(s_t,a_t,s_{t+1})$;\n",
        "\n",
        "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
        "\n",
        "***\n",
        "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRA49fTfl3NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Memory(object):\n",
        "    def __init__(self, max_memory=100):\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = list()\n",
        "\n",
        "    def remember(self, m):\n",
        "        memory_size = len(self.memory)\n",
        "        # if full memory:\n",
        "        # a random element is deleted to make room\n",
        "        if memory_size > self.max_memory:\n",
        "          idx = np.random.randint(memory_size)\n",
        "          self.memory[idx] = m\n",
        "        else:\n",
        "          self.memory.append(m)\n",
        "\n",
        "    def random_access(self):\n",
        "        memory_size = len(self.memory)\n",
        "        return self.memory[np.random.randint(memory_size)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMJ31w7Jl3NP",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "The pipeline we will use for training is given below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QgCnLvRl3NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(agent,env,epoch,prefix=''):\n",
        "    # Number of won games\n",
        "    score = 0\n",
        "    loss = 0\n",
        "\n",
        "    for e in range(epoch):\n",
        "        # At each epoch, we restart to a fresh game and get the initial state\n",
        "        state = env.reset()\n",
        "        # This assumes that the games will terminate\n",
        "        game_over = False\n",
        "\n",
        "        win = 0\n",
        "        lose = 0\n",
        "\n",
        "        while not game_over:\n",
        "            # The agent performs an action\n",
        "            action = agent.act(state)\n",
        "\n",
        "            # Apply an action to the environment, get the next state, the reward\n",
        "            # and if the games end\n",
        "            prev_state = state\n",
        "            state, reward, game_over = env.act(action)\n",
        "\n",
        "            # Update the counters\n",
        "            if reward > 0:\n",
        "                win = win + reward\n",
        "            if reward < 0:\n",
        "                lose = lose -reward\n",
        "\n",
        "            # Apply the reinforcement strategy\n",
        "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
        "\n",
        "        # Save as a mp4\n",
        "        if e % 10 == 0:\n",
        "            env.draw(prefix+str(e))\n",
        "\n",
        "        # Update stats\n",
        "        score += win-lose\n",
        "\n",
        "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
        "              .format(e, epoch, loss, win, lose, win-lose))\n",
        "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd9LViGal3Na",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gla1fMY-l3Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(Agent):\n",
        "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
        "        super(DQN, self).__init__(epsilon = epsilon)\n",
        "\n",
        "        # Discount for Q learning\n",
        "        self.discount = 0.99\n",
        "        \n",
        "        self.grid_size = grid_size\n",
        "        \n",
        "        # number of state\n",
        "        self.n_state = n_state\n",
        "\n",
        "        # Memory\n",
        "        self.memory = Memory(memory_size)\n",
        "        \n",
        "        # Batch size when learning\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def learned_act(self, s):\n",
        "        Q = self.model.predict(np.reshape([s], (1,5,5,self.n_state)))\n",
        "        act = np.argmax(Q)\n",
        "        return act\n",
        "\n",
        "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
        "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
        "        input_states = np.zeros((self.batch_size, 5,5,self.n_state))\n",
        "        target_q = np.zeros((self.batch_size, 4))\n",
        "        \n",
        "        for i in range(self.batch_size):\n",
        "            # Random transition\n",
        "            [s_, n_s_, a_, r_, game_over_] = self.memory.random_access()\n",
        "            input_states[i,:,:,:] = s_\n",
        "            max_Q = self.model.predict(np.array([n_s_]))\n",
        "            a_star = np.argmax(max_Q)\n",
        "            Q = self.model.predict(np.array([s_]))\n",
        "\n",
        "            # Objective function\n",
        "            if game_over_:\n",
        "                target_q[i,:] = Q\n",
        "            else:\n",
        "                target_q[i,:] = Q\n",
        "                target_q[i, a_] = r_ + self.discount*max_Q[0][a_star] - Q[0][a_]\n",
        "\n",
        "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
        "        target_q = np.clip(target_q, -3, 3)\n",
        "        l = self.model.train_on_batch(input_states, target_q)\n",
        "\n",
        "        return l\n",
        "\n",
        "    def save(self,name_weights='model.h5',name_model='model.json'):\n",
        "        self.model.save_weights(name_weights, overwrite=True)\n",
        "        with open(name_model, \"w\") as outfile:\n",
        "            json.dump(self.model.to_json(), outfile)\n",
        "            \n",
        "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
        "        with open(name_model, \"r\") as jfile:\n",
        "            model = model_from_json(json.load(jfile))\n",
        "        model.load_weights(name_weights)\n",
        "        model.compile(\"sgd\", \"mse\")\n",
        "        self.model = model\n",
        "\n",
        "            \n",
        "class DQN_FC(DQN):\n",
        "    def __init__(self, *args, lr=0.1,**kwargs):\n",
        "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
        "        \n",
        "        # NN Model\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Flatten(input_shape=(5, 5, self.n_state)))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(200))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dense(4))\n",
        "        \n",
        "        # model.compile(adam(lr=lr, decay=1e-4), \"mse\")\n",
        "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
        "        self.model = model\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JNsvCxwl3Nm",
        "colab_type": "code",
        "outputId": "6dbf9e48-e7b3-4c90-ad7a-8a9903b4003f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
        "agent = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "train(agent, env, epochs_train, prefix='fc_train')\n",
        "HTML(display_videos('fc_train10.mp4'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 000/035 | Loss 0.0050 | Win/lose count 6.5/11.0 (-4.5)\n",
            "Epoch 001/035 | Loss 0.0030 | Win/lose count 3.0/6.0 (-3.0)\n",
            "Epoch 002/035 | Loss 0.0020 | Win/lose count 3.0/2.0 (1.0)\n",
            "Epoch 003/035 | Loss 0.0060 | Win/lose count 5.0/4.0 (1.0)\n",
            "Epoch 004/035 | Loss 0.0046 | Win/lose count 3.0/1.0 (2.0)\n",
            "Epoch 005/035 | Loss 0.0006 | Win/lose count 4.5/1.0 (3.5)\n",
            "Epoch 006/035 | Loss 0.0016 | Win/lose count 4.0/4.0 (0.0)\n",
            "Epoch 007/035 | Loss 0.0055 | Win/lose count 13.0/4.0 (9.0)\n",
            "Epoch 008/035 | Loss 0.0026 | Win/lose count 11.5/4.0 (7.5)\n",
            "Epoch 009/035 | Loss 0.0040 | Win/lose count 7.0/2.0 (5.0)\n",
            "Epoch 010/035 | Loss 0.0010 | Win/lose count 7.5/4.0 (3.5)\n",
            "Epoch 011/035 | Loss 0.0021 | Win/lose count 7.5/3.0 (4.5)\n",
            "Epoch 012/035 | Loss 0.0005 | Win/lose count 3.0/3.0 (0.0)\n",
            "Epoch 013/035 | Loss 0.0022 | Win/lose count 11.5/2.0 (9.5)\n",
            "Epoch 014/035 | Loss 0.0011 | Win/lose count 8.5/1.0 (7.5)\n",
            "Epoch 015/035 | Loss 0.0024 | Win/lose count 11.5/7.0 (4.5)\n",
            "Epoch 016/035 | Loss 0.0017 | Win/lose count 9.0/4.0 (5.0)\n",
            "Epoch 017/035 | Loss 0.0030 | Win/lose count 11.0/6.0 (5.0)\n",
            "Epoch 018/035 | Loss 0.0023 | Win/lose count 6.0/5.0 (1.0)\n",
            "Epoch 019/035 | Loss 0.0018 | Win/lose count 14.0/2.0 (12.0)\n",
            "Epoch 020/035 | Loss 0.0026 | Win/lose count 4.5/2.0 (2.5)\n",
            "Epoch 021/035 | Loss 0.0019 | Win/lose count 4.0/4.0 (0.0)\n",
            "Epoch 022/035 | Loss 0.0025 | Win/lose count 16.0/2.0 (14.0)\n",
            "Epoch 023/035 | Loss 0.0030 | Win/lose count 4.0/2.0 (2.0)\n",
            "Epoch 024/035 | Loss 0.0032 | Win/lose count 12.5/3.0 (9.5)\n",
            "Epoch 025/035 | Loss 0.0046 | Win/lose count 10.0/2.0 (8.0)\n",
            "Epoch 026/035 | Loss 0.0014 | Win/lose count 8.0/2.0 (6.0)\n",
            "Epoch 027/035 | Loss 0.0025 | Win/lose count 16.0/3.0 (13.0)\n",
            "Epoch 028/035 | Loss 0.0012 | Win/lose count 5.5/3.0 (2.5)\n",
            "Epoch 029/035 | Loss 0.0016 | Win/lose count 11.0/3.0 (8.0)\n",
            "Epoch 030/035 | Loss 0.0040 | Win/lose count 12.5/7.0 (5.5)\n",
            "Epoch 031/035 | Loss 0.0041 | Win/lose count 2.5/0 (2.5)\n",
            "Epoch 032/035 | Loss 0.0020 | Win/lose count 19.5/5.0 (14.5)\n",
            "Epoch 033/035 | Loss 0.0020 | Win/lose count 11.5/1.0 (10.5)\n",
            "Epoch 034/035 | Loss 0.0019 | Win/lose count 15.5/4.0 (11.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SNk2GzNl3Nx",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "***\n",
        "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6mOYOgol3Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN_CNN(DQN):\n",
        "    def __init__(self, *args,lr=0.1,**kwargs):\n",
        "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(50, (2,2)))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Conv2D(100, (2,2)))     \n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(4))\n",
        "\n",
        "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
        "        # model.compile(adam(lr=lr, decay=1e-4), \"mse\")\n",
        "        self.model = model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zLvshXpl3N8",
        "colab_type": "code",
        "outputId": "5b8f08e9-9763-4018-cc24-d4f1f62afcb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        }
      },
      "source": [
        "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
        "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "train(agent,env,epochs_train,prefix='cnn_train')\n",
        "HTML(display_videos('cnn_train10.mp4'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Epoch 000/035 | Loss 0.0038 | Win/lose count 3.5/6.0 (-2.5)\n",
            "Epoch 001/035 | Loss 0.0025 | Win/lose count 2.5/5.0 (-2.5)\n",
            "Epoch 002/035 | Loss 0.0075 | Win/lose count 3.0/7.0 (-4.0)\n",
            "Epoch 003/035 | Loss 0.0061 | Win/lose count 1.5/3.0 (-1.5)\n",
            "Epoch 004/035 | Loss 0.0005 | Win/lose count 6.0/2.0 (4.0)\n",
            "Epoch 005/035 | Loss 0.0079 | Win/lose count 3.5/5.0 (-1.5)\n",
            "Epoch 006/035 | Loss 0.0063 | Win/lose count 6.0/3.0 (3.0)\n",
            "Epoch 007/035 | Loss 0.0071 | Win/lose count 4.0/1.0 (3.0)\n",
            "Epoch 008/035 | Loss 0.0010 | Win/lose count 5.5/1.0 (4.5)\n",
            "Epoch 009/035 | Loss 0.0020 | Win/lose count 4.5/3.0 (1.5)\n",
            "Epoch 010/035 | Loss 0.0020 | Win/lose count 4.0/3.0 (1.0)\n",
            "Epoch 011/035 | Loss 0.0062 | Win/lose count 4.0/4.0 (0.0)\n",
            "Epoch 012/035 | Loss 0.0009 | Win/lose count 5.0/2.0 (3.0)\n",
            "Epoch 013/035 | Loss 0.0035 | Win/lose count 8.0/3.0 (5.0)\n",
            "Epoch 014/035 | Loss 0.0055 | Win/lose count 0.5/3.0 (-2.5)\n",
            "Epoch 015/035 | Loss 0.0126 | Win/lose count 3.0/2.0 (1.0)\n",
            "Epoch 016/035 | Loss 0.0023 | Win/lose count 9.5/5.0 (4.5)\n",
            "Epoch 017/035 | Loss 0.0008 | Win/lose count 6.5/4.0 (2.5)\n",
            "Epoch 018/035 | Loss 0.0044 | Win/lose count 5.0/2.0 (3.0)\n",
            "Epoch 019/035 | Loss 0.0052 | Win/lose count 8.5/2.0 (6.5)\n",
            "Epoch 020/035 | Loss 0.0030 | Win/lose count 4.0/2.0 (2.0)\n",
            "Epoch 021/035 | Loss 0.0050 | Win/lose count 4.0/0 (4.0)\n",
            "Epoch 022/035 | Loss 0.0062 | Win/lose count 3.5/3.0 (0.5)\n",
            "Epoch 023/035 | Loss 0.0017 | Win/lose count 7.5/5.0 (2.5)\n",
            "Epoch 024/035 | Loss 0.0058 | Win/lose count 7.5/2.0 (5.5)\n",
            "Epoch 025/035 | Loss 0.0074 | Win/lose count 10.0/2.0 (8.0)\n",
            "Epoch 026/035 | Loss 0.0075 | Win/lose count 2.5/4.0 (-1.5)\n",
            "Epoch 027/035 | Loss 0.0007 | Win/lose count 10.0/0 (10.0)\n",
            "Epoch 028/035 | Loss 0.0060 | Win/lose count 4.5/1.0 (3.5)\n",
            "Epoch 029/035 | Loss 0.0050 | Win/lose count 10.5/2.0 (8.5)\n",
            "Epoch 030/035 | Loss 0.0009 | Win/lose count 11.0/2.0 (9.0)\n",
            "Epoch 031/035 | Loss 0.0028 | Win/lose count 10.5/2.0 (8.5)\n",
            "Epoch 032/035 | Loss 0.0029 | Win/lose count 9.0/2.0 (7.0)\n",
            "Epoch 033/035 | Loss 0.0013 | Win/lose count 9.0/2.0 (7.0)\n",
            "Epoch 034/035 | Loss 0.0064 | Win/lose count 9.5/3.0 (6.5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ZJr59Vl3OD",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "***\n",
        "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bce1TEwxl3OE",
        "colab_type": "code",
        "outputId": "9c3b3d32-445a-4c92-d5bf-0d5606ad4783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
        "agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
        "\n",
        "agent_fc = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
        "agent_cnn.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
        "print('Test of the CNN')\n",
        "test(agent_cnn,env,epochs_test,prefix='cnn_test')\n",
        "print('Test of the FC')\n",
        "test(agent_fc,env,epochs_test,prefix='fc_test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test of the CNN\n",
            "Win/lose count 18.5/6.0. Average score (12.5)\n",
            "Win/lose count 18.5/2.0. Average score (14.5)\n",
            "Win/lose count 14.5/3.0. Average score (13.5)\n",
            "Win/lose count 7.0/0. Average score (11.875)\n",
            "Win/lose count 10.5/1.0. Average score (11.4)\n",
            "Win/lose count 19.5/2.0. Average score (12.416666666666666)\n",
            "Win/lose count 4.0/3.0. Average score (10.785714285714286)\n",
            "Win/lose count 6.5/0. Average score (10.25)\n",
            "Win/lose count 5.0/4.0. Average score (9.222222222222221)\n",
            "Win/lose count 21.5/3.0. Average score (10.15)\n",
            "Win/lose count 7.0/0. Average score (9.863636363636363)\n",
            "Win/lose count 4.0/6.0. Average score (8.875)\n",
            "Win/lose count 7.5/1.0. Average score (8.692307692307692)\n",
            "Win/lose count 10.0/3.0. Average score (8.571428571428571)\n",
            "Win/lose count 16.0/5.0. Average score (8.733333333333333)\n",
            "Final score: 8.733333333333333\n",
            "Test of the FC\n",
            "Win/lose count 0/2.0. Average score (-2.0)\n",
            "Win/lose count 7.5/5.0. Average score (0.25)\n",
            "Win/lose count 3.5/6.0. Average score (-0.6666666666666666)\n",
            "Win/lose count 2.5/2.0. Average score (-0.375)\n",
            "Win/lose count 5.0/2.0. Average score (0.3)\n",
            "Win/lose count 1.0/2.0. Average score (0.08333333333333333)\n",
            "Win/lose count 6.5/2.0. Average score (0.7142857142857143)\n",
            "Win/lose count 4.0/3.0. Average score (0.75)\n",
            "Win/lose count 4.5/5.0. Average score (0.6111111111111112)\n",
            "Win/lose count 4.0/3.0. Average score (0.65)\n",
            "Win/lose count 5.0/7.0. Average score (0.4090909090909091)\n",
            "Win/lose count 7.0/12.0. Average score (-0.041666666666666664)\n",
            "Win/lose count 7.0/6.0. Average score (0.038461538461538464)\n",
            "Win/lose count 5.5/8.0. Average score (-0.14285714285714285)\n",
            "Win/lose count 6.0/9.0. Average score (-0.3333333333333333)\n",
            "Final score: -0.3333333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44kDjnI6tr1s",
        "colab_type": "text"
      },
      "source": [
        "Both scores beat random. The CNN network achieves better learning than the fully connected network.\n",
        "\n",
        "By playing the videos, we realize that the rats do not explore the entire island. They tend to stay in a certain area once they have eaten all the cheese around them.\n",
        "\n",
        "When we increase the temperature, we increase the proportion of cells containing cheese. Therefore we observe larger score values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPGBz-usl3OK",
        "colab_type": "code",
        "outputId": "fcbf8108-7448-4da6-fb63-d460fd2970e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "HTML(display_videos('cnn_test10.mp4'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9zqeEIwl3OP",
        "colab_type": "code",
        "outputId": "69254c04-6c4a-42f9-9275-4cf90bcc9a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "HTML(display_videos('fc_test10.mp4'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<video alt=\"test\" controls>\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87myc7V4l3OS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfX3VMiel3OT",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
        "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
        "2. Append via the environment a new state that describes if a cell has been visited or not\n",
        "\n",
        "***\n",
        "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWmXR2WRl3OU",
        "colab_type": "code",
        "outputId": "fa91016d-7428-42e2-ef14-2aa348d08afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def train_explore(agent,env,epoch,prefix=''):\n",
        "    pass\n",
        "        \n",
        "class EnvironmentExploring(object):\n",
        "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
        "        pass\n",
        "    \n",
        "## use those samples of code:\n",
        "#In train explore:\n",
        "state, reward, game_over = env.act(action, train=True)\n",
        "\n",
        "## In Environment exploring:\n",
        "# You will have to change n_state to 3 because you will use one more layer!\n",
        "reward = 0\n",
        "if train:\n",
        "    reward = -self.malus_position[self.x, self.y]\n",
        "self.malus_position[self.x, self.y] = 0.1\n",
        "\n",
        "reward = reward + self.board[self.x, self.y]\n",
        "# 3 \"feature\" states instead of 2\n",
        "state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
        "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
        "                        self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7e87314b82d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## use those samples of code:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#In train explore:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m## In Environment exploring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'action' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcV2Dmx5l3Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training\n",
        "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
        "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32,n_state=3)\n",
        "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
        "HTML(display_videos('cnn_train_explore10.mp4'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYcyRVrjl3Oj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluation\n",
        "test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
        "HTML(display_videos('cnn_test_explore10.mp4'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB-rCePxl3Oq",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "***\n",
        "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Qw0RwDl3Os",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZM4oLNl3Ot",
        "colab_type": "text"
      },
      "source": [
        "***"
      ]
    }
  ]
}